{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fba4ba",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b63f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "from nltk.translate import bleu_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.util import ngrams\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "# Set random seed for consistency\n",
    "rand_seed = 7\n",
    "torch.manual_seed(rand_seed)\n",
    "random.seed(rand_seed)\n",
    "np.random.seed(rand_seed)\n",
    "torch.cuda.manual_seed_all(rand_seed)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493d42c",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0cacfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset daily_dialog (C:\\Users\\leoma\\.cache\\huggingface\\datasets\\daily_dialog\\default\\1.0.0\\c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c)\n",
      "Using custom data configuration default\n",
      "Reusing dataset daily_dialog (C:\\Users\\leoma\\.cache\\huggingface\\datasets\\daily_dialog\\default\\1.0.0\\c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c)\n",
      "Using custom data configuration default\n",
      "Reusing dataset daily_dialog (C:\\Users\\leoma\\.cache\\huggingface\\datasets\\daily_dialog\\default\\1.0.0\\c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c)\n"
     ]
    }
   ],
   "source": [
    "# Import DailyDialog dataset\n",
    "train_dataset = datasets.load_dataset(\"daily_dialog\", split=\"train\")\n",
    "test_dataset = datasets.load_dataset(\"daily_dialog\", split=\"test\")\n",
    "val_dataset = datasets.load_dataset(\"daily_dialog\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25535f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['dialog', 'act', 'emotion'],\n",
      "    num_rows: 11118\n",
      "})\n",
      "Dataset({\n",
      "    features: ['dialog', 'act', 'emotion'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['dialog', 'act', 'emotion'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Check datasets\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce41c2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialog': ['Say , Jim , how about going for a few beers after dinner ? ',\n",
       "  ' You know that is tempting but is really not good for our fitness . ',\n",
       "  ' What do you mean ? It will help us to relax . ',\n",
       "  \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \",\n",
       "  \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \",\n",
       "  ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ',\n",
       "  \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \",\n",
       "  ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ',\n",
       "  \" Good.Let ' s go now . \",\n",
       "  ' All right . '],\n",
       " 'act': [3, 4, 2, 2, 2, 3, 4, 1, 3, 4],\n",
       " 'emotion': [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show an example of train data\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c68945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      483\n",
       "3      386\n",
       "4     1980\n",
       "5      942\n",
       "6     1183\n",
       "7      805\n",
       "8     1069\n",
       "9      717\n",
       "10     966\n",
       "11     436\n",
       "12     977\n",
       "13     272\n",
       "14     262\n",
       "15     158\n",
       "16     154\n",
       "17      94\n",
       "18      63\n",
       "19      34\n",
       "20      48\n",
       "21      27\n",
       "22      12\n",
       "23      13\n",
       "24      11\n",
       "25       4\n",
       "26       7\n",
       "27       1\n",
       "28       3\n",
       "29       5\n",
       "30       2\n",
       "31       1\n",
       "32       2\n",
       "35       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the counts of conversation lengths for training set\n",
    "train_dialog_len = [len(dialog) for dialog in train_dataset['dialog']]\n",
    "pd.Series(train_dialog_len).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e41d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2      42\n",
       "3      40\n",
       "4     177\n",
       "5      90\n",
       "6     117\n",
       "7      55\n",
       "8      94\n",
       "9      62\n",
       "10     93\n",
       "11     47\n",
       "12     81\n",
       "13     22\n",
       "14     29\n",
       "15     10\n",
       "16     18\n",
       "17      7\n",
       "18      5\n",
       "19      5\n",
       "20      3\n",
       "21      2\n",
       "26      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the counts of conversation lengths for test set\n",
    "test_dialog_len = [len(dialog) for dialog in test_dataset['dialog']]\n",
    "pd.Series(test_dialog_len).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c38b0d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Are you busy tomorrow morning ? ',\n",
       " \" I'm free . What's up ? \",\n",
       " ' Someone has to pick up the boss at the airport . ',\n",
       " \" Oh , I just remembered I've got a report to write . \"]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a dialog\n",
    "test_dataset['dialog'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ff13e",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee52f93",
   "metadata": {},
   "source": [
    "#### BLEU-1,2,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f609783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to calculate BLEU-1,2,4 score\n",
    "\n",
    "Input:\n",
    "NOTE: All sentence must be SPLIT INTO WORDS\n",
    "ref_list - list of reference sentences, e.g. []\n",
    "hypo - hypothesis sentence\n",
    "\n",
    "Return:\n",
    "BLEU-1 score, BLEU-2 score, BLEU-4 score\n",
    "\"\"\"\n",
    "# Function to calculate BLEU score\n",
    "# Input: \n",
    "def calc_bleu_score(ref_list, prediction):\n",
    "    weights = [\n",
    "         (1, 0),\n",
    "         (1./3., 1./3., 1./3.),\n",
    "         (1./4., 1./4., 1./4., 1./4.)\n",
    "    ]\n",
    "    bleu_scores = bleu_score.sentence_bleu(ref_list, prediction, weights) \n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cfb002",
   "metadata": {},
   "source": [
    "#### ROUGE-1,2,L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12de888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to calculate ROUGE-1,2,L score\n",
    "\n",
    "Input:\n",
    "NOTE: All sentence must be in FULL-SENTENCE STRING format\n",
    "target - target sentence\n",
    "prediction - prediction sentence\n",
    "\n",
    "Return:\n",
    "Dict{'rouge1': ..., 'rouge2': ..., 'rougeL': ...}\n",
    "each key corresponds to:\n",
    "[0] - precision\n",
    "[1] - recall\n",
    "[2] - f1-score\n",
    "\"\"\"\n",
    "\n",
    "def calc_rouge_score(target, prediction):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(target, prediction)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e344e26",
   "metadata": {},
   "source": [
    "#### Distinct-1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd9bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to calculate Distinct-1,2 score\n",
    "\n",
    "Input:\n",
    "NOTE: All sentence must be SPLIT INTO WORDS\n",
    "predictions - a LIST of prediction sentences (from all test data predictions)\n",
    "n - specify n in n-grams\n",
    "\n",
    "Return:\n",
    "distinct score - float\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def calc_distinct_score(predictions, n):\n",
    "    # Get ngrams\n",
    "    ngram_list = []\n",
    "    for prediction in predictions:\n",
    "        line_ngrams = list(ngrams(prediction, n))\n",
    "        ngram_list = ngram_list + line_ngrams\n",
    "    \n",
    "    # Initialize variables\n",
    "    appeared_ngram = []\n",
    "    total = 0\n",
    "    distinct = 0\n",
    "    \n",
    "    # Count number of distinct ngrams\n",
    "    for ngram in ngram_list:\n",
    "        total += 1\n",
    "        if ngram not in appeared_ngram:\n",
    "            distinct += 1\n",
    "            appeared_ngram.append(ngram)\n",
    "    \n",
    "    return distinct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8b173",
   "metadata": {},
   "source": [
    "### DialoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8509037",
   "metadata": {},
   "source": [
    "### Evaluate pre-trained model over the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eedfd4",
   "metadata": {},
   "source": [
    "#### Get evaluation metrics from pre-trained DialoGPT without any fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1e9dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and flatten an input conversation into one single list\n",
    "def tokenize_flatten_conv(conv, tokenizer):\n",
    "    flatten = lambda large_list: [item for sublist in large_list for item in sublist]\n",
    "    conversation = [tokenizer.encode(line) + [tokenizer.eos_token_id] for line in conv] # Tokenize\n",
    "    conversation = flatten(conversation)                                                # Flatten\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0629bc73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate mean score from a list of scores\n",
    "def calc_mean_scores(scores_list):\n",
    "    return np.mean(np.array(scores_list))\n",
    "\n",
    "# Function to calculate evaluation metrics on a given model and dataset\n",
    "# Mode: 'conv' - predict utterance based on previous conversation history\n",
    "#       'sep'  - predict utterance based on only the previous utterance\n",
    "def eval_model_on_metrics(model, tokenizer, test_dataset, mode):\n",
    "\n",
    "    # Initialize lists to store scores and generated texts\n",
    "    bleu_1_scores = []\n",
    "    bleu_2_scores = []\n",
    "    bleu_4_scores = []\n",
    "    rouge_1_scores = []\n",
    "    rouge_2_scores = []\n",
    "    rouge_L_scores = []\n",
    "    distinct = []\n",
    "\n",
    "    # Loop through each dialogue in test dataset\n",
    "    for dialogue in tqdm(test_dataset, desc=\"Evaluation of model\", unit=\"dialogues\"):\n",
    "        for i in range(0, len(dialogue), 2):\n",
    "                \n",
    "            # If the input sentence is the last sentence, skip the prediction\n",
    "            if i == len(dialogue)-1:\n",
    "                continue\n",
    "                \n",
    "            # mode 'conv' (conversation): Use uttr 0 to predict uttr 1, use uttr 0+1+2 to predict uttr 3, ...    \n",
    "            if mode == 'conv':\n",
    "                for j in range(i+1):\n",
    "                    # Add EOS token to the end of each input utterance\n",
    "                    new_input = dialogue[j] + tokenizer.eos_token\n",
    "\n",
    "                    # Convert the sentence into tensor of token ids with the tokenizer\n",
    "                    new_input_ids = tokenizer.encode(new_input, return_tensors='pt').to(device)\n",
    "\n",
    "                    # Concat the new input utterance to the chat history if applicable\n",
    "                    chat_input_ids = torch.cat([chat_input_ids, new_input_ids], dim=-1) if j > 0 else new_input_ids\n",
    "                    \n",
    "            # mode 'sep' (separate): Use uttr 0 to predict uttr 1, use uttr 2 to predict uttr 3, ...\n",
    "            elif mode == 'sep':\n",
    "                # Only use one line to predict\n",
    "                new_input = dialogue[i] + tokenizer.eos_token\n",
    "                \n",
    "                # Convert the sentence into tensor of token ids with the tokenizer\n",
    "                chat_input_ids = tokenizer.encode(new_input, return_tensors='pt').to(device)\n",
    "            \n",
    "            # Get the output token ids from the model\n",
    "            output_ids = model.generate(chat_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            # Convert the output token ids back into a sentence\n",
    "            new_output = tokenizer.decode(output_ids[:, chat_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "        \n",
    "            # Get the target utterance, tokenized output & tokenized target utterance\n",
    "            target_utterance = dialogue[i+1]\n",
    "            \n",
    "            # Split the sentence into words\n",
    "            new_output_split = new_output.split()\n",
    "            target_utterance_split = target_utterance.split()\n",
    "        \n",
    "            # Calculate BLEU-1,2,4 scores and save into lists\n",
    "            bleu_scores = calc_bleu_score([target_utterance_split], new_output_split)\n",
    "            bleu_1_scores.append(bleu_scores[0])\n",
    "            bleu_2_scores.append(bleu_scores[1])\n",
    "            bleu_4_scores.append(bleu_scores[2])\n",
    "        \n",
    "            # Calculate ROUGE-1,2,L scores and save into lists\n",
    "            rouge_scores = calc_rouge_score(target_utterance, new_output)\n",
    "            rouge_1_scores.append(rouge_scores['rouge1'][2])\n",
    "            rouge_2_scores.append(rouge_scores['rouge2'][2])\n",
    "            rouge_L_scores.append(rouge_scores['rougeL'][2])\n",
    "        \n",
    "            # Save the tokenized output into a list for future Distinct score calculation\n",
    "            distinct.append(new_output_split)\n",
    "\n",
    "    # Calculate Distinct-1,2 scores with the saved tokenized outputs\n",
    "    distinct_1_score = calc_distinct_score(distinct, 1)\n",
    "    distinct_2_score = calc_distinct_score(distinct, 2)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(f\"Mean BLEU-1 score: {calc_mean_scores(bleu_1_scores)}\")\n",
    "    print(f\"Mean BLEU-2 score: {calc_mean_scores(bleu_2_scores)}\")\n",
    "    print(f\"Mean BLEU-4 score: {calc_mean_scores(bleu_4_scores)}\")\n",
    "    print(f\"Mean ROUGE-1 F-score: {calc_mean_scores(rouge_1_scores)}\")\n",
    "    print(f\"Mean ROUGE-2 F-score: {calc_mean_scores(rouge_2_scores)}\")\n",
    "    print(f\"Mean ROUGE-L F-score: {calc_mean_scores(rouge_L_scores)}\")\n",
    "    print(f\"Distinct-1 score: {distinct_1_score}\")\n",
    "    print(f\"Distinct-2 score: {distinct_2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b0b3b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get model response output for testing, given a LIST of utterances in a conversation history\n",
    "def chatbot(chat_history, model, tokenizer):\n",
    "    step = 0\n",
    "    for uttr in chat_history:\n",
    "        new_input = uttr + tokenizer.eos_token\n",
    "        new_input_ids = tokenizer.encode(new_input, return_tensors='pt').to(device)\n",
    "        chat_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if step > 0 else new_input_ids\n",
    "    chat_history_ids = model.generate(chat_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    new_output = tokenizer.decode(chat_history_ids[:, chat_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    return new_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b802d5",
   "metadata": {},
   "source": [
    "#### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a0812231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tokenizer & pre-trained DialoGPT model\n",
    "dialogpt_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "dialogpt_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)\n",
    "\n",
    "# Set the padding token to be EOS token\n",
    "dialogpt_tokenizer.pad_token = dialogpt_tokenizer.eos_token\n",
    "#dialogpt_model.config.pad_token_id = dialogpt_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1293a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dialogue inputs\n",
    "test_dialogue1 = [' Can I help you ? ',]\n",
    "test_dialogue2 = ['How are you today ? ',\n",
    " ' Great , thanks . ',\n",
    " ' Can I help you ? ',]\n",
    "test_dialogue3 = [' All right , young man . Tell me how it got started . ']\n",
    "test_dialogue4 = [\"Good morning . What's the matter with you ? \",\n",
    " ' Good morning , doctor . I have a terrible headache . ',\n",
    " ' All right , young man . Tell me how it got started . ',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a8fa873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation of model:   0%|                                                   | 1/1000 [00:04<1:20:36,  4.84s/dialogues]D:\\Anaconda3\\envs\\data_science\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "D:\\Anaconda3\\envs\\data_science\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "D:\\Anaconda3\\envs\\data_science\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Evaluation of model: 100%|██████████████████████████████████████████████████| 1000/1000 [12:25<00:00,  1.34dialogues/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU-1 score: 0.02312607321774004\n",
      "Mean BLEU-2 score: 0.0012744197886526104\n",
      "Mean BLEU-4 score: 0.0004805906812546459\n",
      "Mean ROUGE-1 F-score: 0.09395238010630211\n",
      "Mean ROUGE-2 F-score: 0.01525255405873308\n",
      "Mean ROUGE-L F-score: 0.08921185684966101\n",
      "Distinct-1 score: 0.095533003036599\n",
      "Distinct-2 score: 0.23878256859580355\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance of the model\n",
    "eval_model_on_metrics(dialogpt_model, dialogpt_tokenizer, test_dataset['dialog'], \"conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "764f22b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation of model: 100%|██████████████████████████████████████████████████| 1000/1000 [15:45<00:00,  1.06dialogues/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU-1 score: 0.030905711424916316\n",
      "Mean BLEU-2 score: 0.0013275425407620158\n",
      "Mean BLEU-4 score: 0.00031980848622221417\n",
      "Mean ROUGE-1 F-score: 0.11009679650523381\n",
      "Mean ROUGE-2 F-score: 0.01757537246256902\n",
      "Mean ROUGE-L F-score: 0.10206082506446801\n",
      "Distinct-1 score: 0.08359971202303816\n",
      "Distinct-2 score: 0.21850120870265916\n"
     ]
    }
   ],
   "source": [
    "eval_model_on_metrics(dialogpt_model, dialogpt_tokenizer, test_dataset['dialog'], \"sep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7223f405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure what to do with this information.\n",
      "I'm not sure what to do with this information.\n"
     ]
    }
   ],
   "source": [
    "print(chatbot(test_dialogue1, dialogpt_model, dialogpt_tokenizer))\n",
    "print(chatbot(test_dialogue2, dialogpt_model, dialogpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ee50ca9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a young man, and I'm not sure what you're talking about.\n",
      "I'm a young man, and I'm not sure what you're talking about.\n"
     ]
    }
   ],
   "source": [
    "# Try to get model-generated responses\n",
    "print(chatbot(test_dialogue3, dialogpt_model, dialogpt_tokenizer))\n",
    "print(chatbot(test_dialogue4, dialogpt_model, dialogpt_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca73dc",
   "metadata": {},
   "source": [
    "#### Try larger DialoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "78f8e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "try:\n",
    "    del dialogpt_model\n",
    "    del dialogpt_tokenizer\n",
    "except NameError:\n",
    "    pass\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Import tokenizer & pre-trained DialoGPT model\n",
    "dialogpt_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "dialogpt_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\").to(device)\n",
    "\n",
    "# Set the padding token to be EOS token\n",
    "dialogpt_tokenizer.pad_token = dialogpt_tokenizer.eos_token\n",
    "#dialogpt_model.config.pad_token_id = dialogpt_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92d7a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation of model: 100%|██████████████████████████████████████████████████| 1000/1000 [21:15<00:00,  1.28s/dialogues]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU-1 score: 0.03242453170678008\n",
      "Mean BLEU-2 score: 0.00203300302785073\n",
      "Mean BLEU-4 score: 0.0007096823609430663\n",
      "Mean ROUGE-1 F-score: 0.10993791006540518\n",
      "Mean ROUGE-2 F-score: 0.02592668508612644\n",
      "Mean ROUGE-L F-score: 0.10397484538465324\n",
      "Distinct-1 score: 0.1036715185748425\n",
      "Distinct-2 score: 0.2910552061495458\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance of the model\n",
    "eval_model_on_metrics(dialogpt_model, dialogpt_tokenizer, test_dataset['dialog'], \"conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80e24b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation of model: 100%|██████████████████████████████████████████████████| 1000/1000 [30:51<00:00,  1.85s/dialogues]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU-1 score: 0.0386720532867369\n",
      "Mean BLEU-2 score: 0.001967099045135996\n",
      "Mean BLEU-4 score: 0.0008031037805097597\n",
      "Mean ROUGE-1 F-score: 0.1290583710952275\n",
      "Mean ROUGE-2 F-score: 0.027180262753334586\n",
      "Mean ROUGE-L F-score: 0.12064876074836307\n",
      "Distinct-1 score: 0.08578091330401157\n",
      "Distinct-2 score: 0.24273453292242436\n"
     ]
    }
   ],
   "source": [
    "eval_model_on_metrics(dialogpt_model, dialogpt_tokenizer, test_dataset['dialog'], \"sep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "56a6550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure what you mean by help.\n",
      "I'm not sure what you mean by help.\n"
     ]
    }
   ],
   "source": [
    "# Try to get model-generated responses\n",
    "print(chatbot(test_dialogue1, dialogpt_model, dialogpt_tokenizer))\n",
    "print(chatbot(test_dialogue2, dialogpt_model, dialogpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "85e4b2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not your young man, young man.\n",
      "I'm not your young man, young man.\n"
     ]
    }
   ],
   "source": [
    "print(chatbot(test_dialogue3, dialogpt_model, dialogpt_tokenizer))\n",
    "print(chatbot(test_dialogue4, dialogpt_model, dialogpt_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6184e2",
   "metadata": {},
   "source": [
    "### Fine-tune pre-trained model with DailyDialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "427e2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize dataset into a LIST of dialogue tensors with tokenized ids\n",
    "def tokenize_dataset(dataset, max_conv_len, dialogpt_tokenizer):\n",
    "    output_list = []\n",
    "    for dialogue in tqdm(dataset, desc=\"Tokenizing and flattening dialogues\", unit=\"dialogues\"):\n",
    "        tokenized_conv = tokenize_flatten_conv(dialogue, dialogpt_tokenizer) # Tokenize & flatten\n",
    "        tokenized_conv = torch.tensor(tokenized_conv)                        # Turn tokenized output into tensor\n",
    "        \n",
    "        # Truncate the tokenized output tensor if it exceeds the maximum conversation length\n",
    "        if len(tokenized_conv) > max_conv_len:\n",
    "            tokenized_conv = tokenized_conv[:max_conv_len]\n",
    "            \n",
    "        output_list.append(tokenized_conv)  # Save the (truncated) conversation into a list\n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a2be818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function to pad the conversation to maximum conversation length\n",
    "def collate(conv):\n",
    "    \n",
    "    ## NOTE: dialogpt_tokenizer referenced on the cell above during model loading\n",
    "    return pad_sequence(conv, batch_first=True, padding_value=dialogpt_tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be42538",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a353fd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model save path\n",
    "model_output_dir = './models/'\n",
    "\n",
    "# Function to fine-tune a DialoGPT model\n",
    "def fine_tune_dialogpt(dialogpt_model, dialogpt_tokenizer):\n",
    "    \n",
    "    n_epochs = 20\n",
    "    max_conv_len = 128\n",
    "    batch_size = 8\n",
    "    best_perplexity = 100\n",
    "    early_stop = 3\n",
    "    no_improve_count = 0\n",
    "\n",
    "    # Tokenize training & validation datasets\n",
    "    train_data_list = tokenize_dataset(train_dataset['dialog'], max_conv_len, dialogpt_tokenizer)\n",
    "    val_data_list = tokenize_dataset(val_dataset['dialog'], max_conv_len, dialogpt_tokenizer)\n",
    "\n",
    "    # Create DataLoaders for training & validation datasets\n",
    "    train_dataloader = DataLoader(train_data_list, batch_size=batch_size, shuffle=True, collate_fn=collate, drop_last=True)\n",
    "    val_dataloader = DataLoader(val_data_list, batch_size=8, shuffle=False, collate_fn=collate, drop_last=False)\n",
    "\n",
    "    # Define optimizer & scheduler\n",
    "    optimizer = torch.optim.AdamW(dialogpt_model.parameters(), lr=5e-5)\n",
    "    total_num_of_steps = len(train_dataloader)*n_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, total_num_of_steps // 10, total_num_of_steps)\n",
    "\n",
    "    # Loop through each epoch\n",
    "    for n in range(n_epochs):\n",
    "    \n",
    "        # Initialize training phase\n",
    "        train_loss = 0.0\n",
    "        eval_loss = 0.0\n",
    "        dataloader_len = len(train_dataloader)\n",
    "        print(f\"Running epoch {n+1}...\")\n",
    "        dialogpt_model.train()\n",
    "    \n",
    "        # Training phase\n",
    "        for batch in tqdm(train_dataloader, desc=\"Fine-tuning the model\", unit=\"batch\"):\n",
    "        \n",
    "            # Pass the inputs and labels to device\n",
    "            inputs, labels = (batch, batch)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = inputs.to(device)\n",
    "        \n",
    "            # Pass to the model and get the loss\n",
    "            outputs = dialogpt_model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # the output is a tuple, the first element is the loss\n",
    "        \n",
    "            # Backpropagation\n",
    "            dialogpt_model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "            # Save training loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate mean training loss\n",
    "        avg_train_loss = train_loss / dataloader_len\n",
    "        print(f\"Train_loss: {avg_train_loss}\")\n",
    "    \n",
    "        # Evaluation phase\n",
    "        dialogpt_model.eval()\n",
    "        for batch in tqdm(val_dataloader, desc=\"Evaluating the model\", unit=\"batch\"):\n",
    "        \n",
    "            # Pass the inputs and labels to device\n",
    "            inputs, labels = (batch, batch)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = inputs.to(device)\n",
    "        \n",
    "            # Pass to the model and get the loss\n",
    "            with torch.no_grad():\n",
    "                outputs = dialogpt_model(inputs, labels=labels)\n",
    "                eval_loss += outputs[0].item()\n",
    "    \n",
    "        # Calculate perplexity\n",
    "        avg_eval_loss = eval_loss / len(val_dataloader)\n",
    "        perplexity = torch.exp(torch.tensor(avg_eval_loss))\n",
    "        print(f\"Perplexity: {perplexity}\")\n",
    "    \n",
    "        # Save the model if it is the current best one\n",
    "        if perplexity < best_perplexity:\n",
    "            best_perplexity = perplexity\n",
    "            dialogpt_model.save_pretrained(model_output_dir)  # Save the model\n",
    "            print(f\"Saved best model.    Epoch: {n+1}    Perplexity: {perplexity}\")\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "    \n",
    "        # Early stop\n",
    "        if no_improve_count == early_stop:\n",
    "            print(\"Stopping training since eval score didn't improve for {early_stop} epochs.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f26528",
   "metadata": {},
   "source": [
    "#### Fine-tune the model & evaluate the fine-tuned model on evaluation metrics on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cd7b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing and flattening dialogues: 100%|███████████████████████████████| 11118/11118 [00:12<00:00, 892.12dialogues/s]\n",
      "Tokenizing and flattening dialogues: 100%|█████████████████████████████████| 1000/1000 [00:01<00:00, 821.32dialogues/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:36<00:00,  5.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.6811932248979957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:06<00:00, 20.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 7.937488079071045\n",
      "Saved best model.    Epoch: 1    Perplexity: 7.937488079071045\n",
      "Running epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:36<00:00,  5.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 2.005671069637488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:06<00:00, 20.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 6.8491692543029785\n",
      "Saved best model.    Epoch: 2    Perplexity: 6.8491692543029785\n",
      "Running epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:42<00:00,  4.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.8424023593218743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:06<00:00, 19.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 6.379498481750488\n",
      "Saved best model.    Epoch: 3    Perplexity: 6.379498481750488\n",
      "Running epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:47<00:00,  4.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.718144028327549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:06<00:00, 19.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 6.116057872772217\n",
      "Saved best model.    Epoch: 4    Perplexity: 6.116057872772217\n",
      "Running epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:42<00:00,  4.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.6138428967577507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:05<00:00, 21.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.9696044921875\n",
      "Saved best model.    Epoch: 5    Perplexity: 5.9696044921875\n",
      "Running epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:22<00:00,  5.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.5229063176096043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:05<00:00, 21.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.8800048828125\n",
      "Saved best model.    Epoch: 6    Perplexity: 5.8800048828125\n",
      "Running epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:37<00:00,  5.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.441306722902239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:05<00:00, 21.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.8708295822143555\n",
      "Saved best model.    Epoch: 7    Perplexity: 5.8708295822143555\n",
      "Running epoch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:28<00:00,  5.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.367401417551322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:06<00:00, 20.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.81952428817749\n",
      "Saved best model.    Epoch: 8    Perplexity: 5.81952428817749\n",
      "Running epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:46<00:00,  4.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.3014992149379119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:06<00:00, 18.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.88892126083374\n",
      "Running epoch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:49<00:00,  4.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.2434352300766134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:06<00:00, 18.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.870273590087891\n",
      "Running epoch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model: 100%|████████████████████████████████████████████████████| 1389/1389 [04:47<00:00,  4.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss: 1.1918842951285058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 100%|███████████████████████████████████████████████████████| 125/125 [00:06<00:00, 19.43batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.91548490524292\n",
      "Stopping training since eval score didn't improve for {early_stop} epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Free memory\n",
    "try:\n",
    "    del dialogpt_model\n",
    "    del dialogpt_tokenizer\n",
    "except NameError:\n",
    "    pass\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load tokenizer and initialize the EOS token as PAD token\n",
    "dialogpt_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "dialogpt_tokenizer.pad_token = dialogpt_tokenizer.eos_token\n",
    "\n",
    "# Import pre-trained DialoGPT model & define PAD token\n",
    "dialogpt_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)\n",
    "#dialogpt_model.config.pad_token_id = dialogpt_model.config.eos_token_id\n",
    "\n",
    "# Run the training function\n",
    "fine_tune_dialogpt(dialogpt_model, dialogpt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a144cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "try:\n",
    "    del dialogpt_model\n",
    "except NameError:\n",
    "    pass\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load best saved model\n",
    "dialogpt_model = AutoModelForCausalLM.from_pretrained(model_output_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db17bbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation of model: 100%|██████████████████████████████████████████████████| 1000/1000 [20:50<00:00,  1.25s/dialogues]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU-1 score: 0.04890141882678121\n",
      "Mean BLEU-2 score: 0.008890029526589923\n",
      "Mean BLEU-4 score: 0.0038604564463968903\n",
      "Mean ROUGE-1 F-score: 0.1435712952922434\n",
      "Mean ROUGE-2 F-score: 0.048553571879876384\n",
      "Mean ROUGE-L F-score: 0.1367040996430867\n",
      "Distinct-1 score: 0.0875288328463786\n",
      "Distinct-2 score: 0.2431566882416397\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance of the model\n",
    "eval_model_on_metrics(dialogpt_model, dialogpt_tokenizer, test_dataset['dialog'], \"conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0c95aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation of model: 100%|██████████████████████████████████████████████████| 1000/1000 [17:52<00:00,  1.07s/dialogues]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU-1 score: 0.053865721621866126\n",
      "Mean BLEU-2 score: 0.006653280556751681\n",
      "Mean BLEU-4 score: 0.00227100247652534\n",
      "Mean ROUGE-1 F-score: 0.1649563729158313\n",
      "Mean ROUGE-2 F-score: 0.05005987018899637\n",
      "Mean ROUGE-L F-score: 0.15516420030494096\n",
      "Distinct-1 score: 0.10102463932010555\n",
      "Distinct-2 score: 0.30718851719675283\n"
     ]
    }
   ],
   "source": [
    "eval_model_on_metrics(dialogpt_model, dialogpt_tokenizer, test_dataset['dialog'], \"sep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4a606c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, I'd like to buy a new shirt. \n",
      " Yes, I'd like to buy a new shirt. \n"
     ]
    }
   ],
   "source": [
    "# Try to get model-generated responses\n",
    "print(chatbot(test_dialogue1, dialogpt_model, dialogpt_tokenizer))\n",
    "print(chatbot(test_dialogue2, dialogpt_model, dialogpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "df221693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Well, first of all, it was a lot of work. \n",
      " Well, first of all, it was a lot of work. \n"
     ]
    }
   ],
   "source": [
    "print(chatbot(test_dialogue3, dialogpt_model, dialogpt_tokenizer))\n",
    "print(chatbot(test_dialogue4, dialogpt_model, dialogpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42ac3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
